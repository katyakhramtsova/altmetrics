{
    "contents" : "---\ntitle: \"on the origins of citations\"\nauthor: \"katya\"\ndate: \"Tuesday, September 15, 2015\"\noutput:\n  pdf_document:\n    toc: yes\n  html_document:\n    fig_caption: yes\n    fig_height: 8\n    fig_width: 8\n    highlight: espresso\n    number_sections: yes\n    self_contained: no\n    theme: cerulean\n    toc: yes\n  word_document: default\n---\n\n# Load the data\n\n## using read.delim\n\n```{r load_data}\ncounts_raw <- read.delim(\"data/counts-raw.txt.gz\")\ncounts_norm <- read.delim(\"data/counts-norm.txt.gz\")\n```\n\n# Data exploration\n\nWhat's the distribution of authors in all articles of our data set?\n```{r author_hist, fig.cap=\"Figure 1: Number of Authors per Article\", echo=FALSE}\nhist(counts_raw$authorsCount, main=\"Authors per paper\", xlab=\"# authors\", ylab=\"frequency\")\n```\n\n```{r facebook_hist, fig.cap=\"Figure 2: Number of Facebook Shares per Article\", echo=FALSE}\nhist(counts_raw$facebookShareCount, main=\"Facebook shares per paper\", xlab=\"# shares\", ylab=\"frequency\")\n```\n\nThe average number of Facebook shares per paper in the data set is `r mean(counts_raw$facebookShareCount)[1]`\n\n##dplyr\n\n```{r}\nlibrary(\"dplyr\")\n```\n\n```{r}\nresearch <- filter(counts_raw, articleType == \"Research Article\")\n```\n\n```{r}\nresearch_2006 <- filter(research, year == 2006)\nnrow(research_2006)\n```\n\n```{r}\nresearch_2006_tweet <- filter(research_2006, backtweetsCount > 0)\nnrow(research_2006_tweet)\n```\n\n```{r}\nresearch_2006_fb <- filter(research, year == 2006,\n                           facebookCommentCount > 0)\nnrow(research_2006_fb)\n```\n\n```{r}\nresearch_2006_fb_tweet <- filter(research, year == 2006,\n                                 facebookCommentCount > 0 |\n                                 backtweetsCount > 0)\nnrow(research_2006_fb_tweet)\n```\n\n```{r}\nresearch_2006_fb_tweet_disease <- filter(research, year == 2006,\n                                         facebookCommentCount > 0 |\n                                         backtweetsCount > 0,\n                                         grepl(\"Infectious Diseases\",\n                                               plosSubjectTags))\nnrow(research_2006_fb_tweet_disease)\n```\n\n```{r}\ncolnames(research)\n```\n\n```{r}\narticle_info <- select(research, doi, pubDate, journal, title,\n                       articleType, authorsCount)\ncolnames(article_info)\n```\n\n```{r}\narticle_info <- select(research, doi:authorsCount)\ncolnames(article_info)\n```\n\n```{r}\nmetrics <- select(research, contains(\"Count\"))\ncolnames(metrics)\n```\n\n```{r}\nmetrics <- select(research, contains(\"Count\"), -authorsCount)\ncolnames(metrics)\n```\n\n```{r}\nmetrics <- select(research, contains(\"Count\"), -authorsCount,\n                  f1000Factor, wikipediaCites)\ncolnames(metrics)\n```\n\n```{r}\nhead(select(research, journal))\nhead(select(research, 3))\n```\n\n```{r}\nslice(article_info, 1:3)\n```\n\n\n```{r}\nlow_cite <- filter(research, year <= 2008,pdfDownloadsCount >  1000, mendeleyReadersCount > 15, wosCountThru2011 < 10)\nlength(low_cite)\n```\n\n\n```{r}\nlow_cite <- filter(research, year <= 2008, pdfDownloadsCount >  1000, mendeleyReadersCount > 15, wosCountThru2011 < 10)\nnrow(low_cite)\nlength(low_cite)\nselect(low_cite, title)\n```\n\n### Chaining commands with dplyr\n\npipe character %>%\n\n```{r}\nfacebook_2006 <- research %>% filter(year == 2006) %>%\n  select(contains(\"facebook\"))\nhead(facebook_2006)\n\nresearch %>% filter(year == 2006) %>%\n  select(contains(\"facebook\")) %>% nrow\n```\n\narrange, work similar to function order\n```{r}\nresearch %>%\n  arrange(authorsCount, wosCountThru2011) %>%\n  select(authorsCount, wosCountThru2011) %>%\n  slice(1:10)\n```\n\n```{r}\nresearch %>%\n  arrange(desc(authorsCount), desc(wosCountThru2011)) %>%\n  select(authorsCount, wosCountThru2011) %>%\n  slice(1:10)\n```\n\nUsing a chain of pipes, output the titles of the three research articles with the largest 2011 citation count.\n```{r, Title of most cited articles}\nresearch %>% arrange(desc(authorsCount), desc(wosCountThru2011), desc(title))  %>% select(title) %>% slice(1:3)\n```\nCorrect solution solution\n```{r}\nreseach %>% arrange(desc(wosCountThru2011)) %>% slice(1:3) %>% select(title)\n```\n\n\nUsing a chain of pipes, output the author count, title, journal, and subject tags (plosSubjectTags) of the three research articles with the largest number of authors.\n```{r, Lots of authors}\nresearch %>% arrange(desc(authorsCount), desc(title), desc(journal), desc(plosSubjectTags))  %>% select(authorsCount, title, journal, plosSubjectTags) %>% slice(1:3)\n```\nCorrect solution solution\n```{r}\nresearch %>% arrange(desc(authorCount)) %>% select(authorsCount, title, journal, plosSubjectTags) %>% slice(1:3) \n```\n\n###summarizing with dplyr\n\n```{r}\nresearch <- mutate(research,\n                   weeksSincePublished = daysSincePublished / 7,\n                   yearsSincePublished = weeksSincePublished / 52)\nselect(research, contains(\"Since\")) %>% slice(1:10)\nresearch %>% select(contains(\"Since\")) %>% slice(1:10)\n```\n\nusing summarize\n\n```{r}\nresearch %>% summarize(research, plos_mean = mean(plosCommentCount))\n```\n\n```{r}\nresearch %>% group_by(journal, year) %>% summarize(tweets_mean = mean(backtweetsCount))\n```\n\nCreate a new data frame, tweets_per_journal, that for each journal contains the total number of articles, the mean number of tweets received by articles in that journal, and the standard error of the mean (SEM) of the number of tweets. The SEM is the standard deviation divided by the square root of the sample size (i.e. the number of articles).\n\n```{r Summarizing the number of tweets per journal}\ntweets_per_journal <- group_by(journal) %>% summarize(num_articles= , tweets_mean = mean(backtweetsCount), SEM = (stdev(backtweetsCount))/sqrt(num_articles))\n```\nCorrect answer\n```{r}\ntweets_per_journal <- research %>% group_by(journal) %>% summarize(num= n() , mean = mean(backtweetsCount), sem = (sd(backtweetsCount))/sqrt(num))\n```\n\n#ggplot\n\n```{r}\nlibrary(\"ggplot\")\n```\n\n```{r}\np <- ggplot(data = research, mapping = aes(x = pdfDownloadsCount,\n                                           y = wosCountThru2011)) + \n  geom_point(aes(color = journal))\np\n```\n\n```{r}\np <- ggplot(research, aes(x = pdfDownloadsCount,\n                          y = wosCountThru2011)) +\n  geom_point(aes(size = authorsCount))\np\n```\n\n```{r}\np <- ggplot(research, aes(x = pdfDownloadsCount,\n                          y = wosCountThru2011)) +\n  geom_point(aes(alpha = daysSincePublished))\np\n```\n\n```{r}\np <- ggplot(research, aes(x = pdfDownloadsCount,\n                          y = wosCountThru2011)) +\n  geom_point(aes(color = journal)) +\n  geom_smooth()\np\n```\n\nCreate a scatter plot with daysSincePublished mapped to the x-axis and wosCountThru2011 mapped to the y-axis. Include a loess fit of the data. Set the transparency level (alpha) of the points to 0.5 and color the points according to the journal where the article was published. Make the loess curve red.\n```{r}\np <- ggplot(research, aes(x = daysSincePublished,\n                          y = wosCountThru2011)) +\n  geom_point(aes(color=journal), alpha =0.5) +\n  geom_smooth(color=\"red\")\np\n```\n###Using scales\n\n```{r}\np <- ggplot(research, aes(x = log10(pdfDownloadsCount + 1),\n                          y = log10(wosCountThru2011 + 1))) +\n  geom_point(aes(color = journal)) +\n  geom_smooth() +\n  scale_x_continuous(breaks = c(1, 3), labels = c(10, 1000)) +\n  scale_y_continuous(breaks = c(1, 3), labels = c(10, 1000),\n  limits=c(1,3))\np\n```\n\n```{r}\np + scale_color_grey()\np + scale_color_manual(values = c(\"red\", \"yellow\", \"orange\",\n                                  \"purple\", \"blue\", \"yellow\",\n                                  \"pink\"))\n```\n\nUpdate the plot to use a square root transformation instead of log10. Also color the points using the ColorBrewer palette “Accent”.\n\n```{r}\np <- ggplot(research, aes(x = sqrt(pdfDownloadsCount),\n                          y = sqrt(wosCountThru2011))) +\n  geom_point(aes(color = journal)) +\n  geom_smooth() + scale_color_brewer(palette = \"Accent\", labels = 1:7, name = \"title\")\np\n```\n\n###Using facets to make subplots\n\n```{r}\np <- ggplot(research, aes(x = sqrt(pdfDownloadsCount),\n                          y = sqrt(wosCountThru2011))) +\n  geom_point(aes(color = journal)) +\n  geom_smooth() + scale_color_brewer(palette = \"Accent\") + facet_wrap(~journal, ncol=2)\np\n```\nusing facet_grid\n\n```{r}\nresearch <- mutate(research, immuno = grepl(\"Immunology\", plosSubjectTags))\np + facet_grid(journal~immuno)\n```\n\n```{r}\np <- ggplot(research, aes(x = sqrt(pdfDownloadsCount),\n                          y = sqrt(wosCountThru2011))) +\n  geom_point(aes(color = journal)) +\n  geom_smooth() + scale_color_brewer(palette = \"Accent\") + facet_grid(journal~immuno)\np\n```\n\n### Using different geoms\n\n```{r}\np <- ggplot(research, aes(x = journal,\n                          y = sqrt(wosCountThru2011))) +\n  geom_boxplot()\np\n```\n\nmaking a bar plot\n\n```{r}\ntweets_per_journal <- research %>%\n  group_by(journal) %>%\n  summarize(num = n(),\n            mean = mean(backtweetsCount),\n            sem = sd(backtweetsCount) / sqrt(num))\ntweets_per_journal\n```\n\n```{r}\ntweets_bar <- ggplot(tweets_per_journal, aes(x = journal, y = mean)) +\n  geom_bar(stat = \"identity\") +\n  geom_errorbar(aes(ymin = mean - sem, ymax = mean + sem), width = 0.1) +\n  geom_text(aes(label = num), hjust = 0, vjust = -1)\ntweets_bar\n```\n\nModify the dplyr code above to calculate the mean, SEM, and sample size of the number of article tweets per journal and per year. Use facet_wrap to make a separate subplot per year.\n\nCorrect solution:\n```{r}\ntweets_per_journal <- research %>%\n  group_by(journal, year) %>%\n  summarize(num = n(),\n            mean = mean(backtweetsCount),\n            sem = sd(backtweetsCount) / sqrt(num))\ntweets_per_journal\n```\n\n```{r}\ntweets_bar <- ggplot(tweets_per_journal, aes(x = journal, y = mean)) +\n  geom_bar(stat = \"identity\") +\n  geom_errorbar(aes(ymin = mean - sem, ymax = mean + sem), width = 0.1) +\n  geom_text(aes(label = num), hjust = 0, vjust = -1) +\n  facet_wrap(~year)\ntweets_bar\n```\n\nMaking a poit grpah\n```{r}\ntweets_bar <- ggplot(tweets_per_journal, aes(x = journal, y = mean)) +\n  geom_point(stat = \"identity\") +\n  geom_errorbar(aes(ymin = mean - sem, ymax = mean + sem), width = 0.1) +\n  geom_text(aes(label = num), hjust = 0, vjust = -1) +\n  facet_wrap(~year)\ntweets_bar\n```\n### Customizing plot\n\n```{r}\ntweets_bar + labs(title=\"sdf\", x= \"asd\", y=\"asd\") + theme_minimal\ntweets_bar\ntweets_bar + theme_bw()\ntweets_bar + theme_classic()\n```\n\n",
    "created" : 1442332835343.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2749931836",
    "id" : "F3BDAA28",
    "lastKnownWriteTime" : 1442352432,
    "path" : "C:/Users/Ekaterina/altmetrics/altmetrics/on_the_origins_of_citations.Rmd",
    "project_path" : "on_the_origins_of_citations.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}